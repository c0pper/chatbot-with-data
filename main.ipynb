{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from os import environ\n",
    "env_file = '.env'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv(env_file, override=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"data\\e274394a0bfb4de196edae9a00adda36.pdf\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\condivisi\\chatbot-with-data\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_directory = \"db\"\n",
    "\n",
    "def init_chromadb():\n",
    "    client_settings = chromadb.config.Settings(\n",
    "        chroma_db_impl=\"duckdb+parquet\",\n",
    "        persist_directory=persist_directory,\n",
    "        anonymized_telemetry=False\n",
    "    )\n",
    "\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=\"langchain_store\",\n",
    "        embedding_function=embeddings,\n",
    "        client_settings=client_settings,\n",
    "        persist_directory=persist_directory,\n",
    "    )\n",
    "\n",
    "    if os.path.exists(persist_directory) and os.path.isdir(persist_directory):\n",
    "        print(\"Directory 'db' exists. Using existing vectordb\")\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Directory 'db' does not exist. Creating new vectordb\")\n",
    "        vectorstore.add_documents(documents=docs, embedding=embeddings)\n",
    "        vectorstore.persist()\n",
    "    \n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chromadb(vectorstore):\n",
    "    result = vectorstore.similarity_search_with_score(query=\"How to turn on the machine?\", k=4)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: db\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'db' exists. Using existing vectordb\n"
     ]
    }
   ],
   "source": [
    "vectorestore = init_chromadb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_chromadb(vectorestore)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LLM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema nel caricamento di modelli grandi\n",
    "\n",
    "| HF Model                | Esito                                                                           | Size |\n",
    "|-------------------------|---------------------------------------------------------------------------------|------|\n",
    "| google/flan-t5-small    | ok                                                                              |300MB |\n",
    "| google/flan-t5-xl       |ValueError: Error raised by inference API: Model {model name} time out           |12GB  |\n",
    "| databricks/dolly-v2-12b |ValueError: Error raised by inference API: Model {model name} time out           |24GB  |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model from huggingface"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Load model from disk](https://github.com/hwchase17/langchain/issues/2667#issuecomment-1501967127)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### flan-T5-xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.84s/it]\n",
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_path = \"D:\\\\hf_models\\\\flan-t5-xl\"\n",
    "tokenizer_path = \"D:\\\\hf_models\\\\flan-t5-xl\"\n",
    "\n",
    "tokenizer = T5Tokenizer(\n",
    "    vocab_file=f\"{tokenizer_path}/spiece.model\",\n",
    "    tokenizer_file=f\"{tokenizer_path}/spiece.model\",\n",
    "    config_file=f\"{tokenizer_path}/tokenizer_config.json\",\n",
    "    use_fast=False\n",
    ")\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "# text_generator = pipeline(\"text-generation\", model=model, max_new_tokens=64, model_kwargs={\"temperature\":0}, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write something about you thing about: / something about: / something about: / something about: / something about: / something about: / something about: / something about: / something about\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "input_text = \"Write something about you \"\n",
    "\n",
    "generated_text = llm(input_text, max_length=50, num_return_sequences=1)[0]['generated_text']\n",
    "print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### flan-alpaca-gpt4-xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type t5 to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 180355072 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 15\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39m# tokenizer = AutoTokenizer(\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m#     vocab_file=f\"{tokenizer_path}/spiece.model\",\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m#     tokenizer_file=f\"{tokenizer_path}/spiece.model\",\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m# model = LlamaForCausalLM.from_pretrained(model_path)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m tokenizer \u001b[39m=\u001b[39m T5Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_path)\n\u001b[1;32m---> 15\u001b[0m model \u001b[39m=\u001b[39m LlamaForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_path)\n\u001b[0;32m     17\u001b[0m text_generator \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39m\u001b[39mtext-generation\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m=\u001b[39mmodel, tokenizer\u001b[39m=\u001b[39mtokenizer)\n",
      "File \u001b[1;32mc:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\transformers\\modeling_utils.py:2629\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2626\u001b[0m     init_contexts\u001b[39m.\u001b[39mappend(init_empty_weights())\n\u001b[0;32m   2628\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m-> 2629\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(config, \u001b[39m*\u001b[39mmodel_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2631\u001b[0m \u001b[39m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[0;32m   2632\u001b[0m \u001b[39mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "File \u001b[1;32mc:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:614\u001b[0m, in \u001b[0;36mLlamaForCausalLM.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config):\n\u001b[0;32m    613\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(config)\n\u001b[1;32m--> 614\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m LlamaModel(config)\n\u001b[0;32m    616\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39mhidden_size, config\u001b[39m.\u001b[39mvocab_size, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    618\u001b[0m     \u001b[39m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:445\u001b[0m, in \u001b[0;36mLlamaModel.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mvocab_size\n\u001b[0;32m    444\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mvocab_size, config\u001b[39m.\u001b[39mhidden_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_idx)\n\u001b[1;32m--> 445\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([LlamaDecoderLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[0;32m    446\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39m=\u001b[39m LlamaRMSNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n\u001b[0;32m    448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:445\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mvocab_size\n\u001b[0;32m    444\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mvocab_size, config\u001b[39m.\u001b[39mhidden_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_idx)\n\u001b[1;32m--> 445\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([LlamaDecoderLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[0;32m    446\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39m=\u001b[39m LlamaRMSNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n\u001b[0;32m    448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:256\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mhidden_size\n\u001b[0;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn \u001b[39m=\u001b[39m LlamaAttention(config\u001b[39m=\u001b[39mconfig)\n\u001b[1;32m--> 256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp \u001b[39m=\u001b[39m LlamaMLP(\n\u001b[0;32m    257\u001b[0m     hidden_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_size,\n\u001b[0;32m    258\u001b[0m     intermediate_size\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mintermediate_size,\n\u001b[0;32m    259\u001b[0m     hidden_act\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mhidden_act,\n\u001b[0;32m    260\u001b[0m )\n\u001b[0;32m    261\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm \u001b[39m=\u001b[39m LlamaRMSNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n\u001b[0;32m    262\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_layernorm \u001b[39m=\u001b[39m LlamaRMSNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[1;32mc:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:151\u001b[0m, in \u001b[0;36mLlamaMLP.__init__\u001b[1;34m(self, hidden_size, intermediate_size, hidden_act)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    145\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    146\u001b[0m     hidden_size: \u001b[39mint\u001b[39m,\n\u001b[0;32m    147\u001b[0m     intermediate_size: \u001b[39mint\u001b[39m,\n\u001b[0;32m    148\u001b[0m     hidden_act: \u001b[39mstr\u001b[39m,\n\u001b[0;32m    149\u001b[0m ):\n\u001b[0;32m    150\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m--> 151\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgate_proj \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(hidden_size, intermediate_size, bias\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    152\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_proj \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(intermediate_size, hidden_size, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup_proj \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(hidden_size, intermediate_size, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features \u001b[39m=\u001b[39m in_features\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features \u001b[39m=\u001b[39m out_features\n\u001b[1;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty((out_features, in_features), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n\u001b[0;32m     97\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[0;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(out_features, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 180355072 bytes."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, LlamaForCausalLM, T5Tokenizer\n",
    "\n",
    "model_path = \"D:\\\\hf_models\\\\flan-alpaca-gpt4-xl\"\n",
    "tokenizer_path = \"D:\\\\hf_models\\\\flan-alpaca-gpt4-xl\"\n",
    "\n",
    "# tokenizer = AutoTokenizer(\n",
    "#     vocab_file=f\"{tokenizer_path}/spiece.model\",\n",
    "#     tokenizer_file=f\"{tokenizer_path}/spiece.model\",\n",
    "#     config_file=f\"{tokenizer_path}/tokenizer_config.json\",\n",
    "#     use_fast=False\n",
    "# )\n",
    "# model = LlamaForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "model = LlamaForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RuntimeError: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 180355072 bytes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### flan-alpaca-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type t5 to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\simone.marotta\\AppData\\Local\\Temp\\3\\ipykernel_12372\\3992398071.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">17</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\simone.marotta\\\\AppData\\\\Local\\\\Temp\\\\3\\\\ipykernel_12372\\\\3992398071.py'</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\transformers\\mo</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">deling_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2611</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2608 │   │   │   </span>init_contexts.append(init_empty_weights())                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2609 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2610 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> ContextManagers(init_contexts):                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2611 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>model = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>(config, *model_args, **model_kwargs)                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2612 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2613 │   │   # Check first if we are `from_pt`</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2614 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> use_keep_in_fp32_modules:                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\transformers\\mo</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">dels\\llama\\modeling_llama.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">615</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">612 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">class</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; text-decoration: underline\">LlamaForCausalLM</span>(LlamaPreTrainedModel):                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">613 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, config):                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">614 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>().<span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>(config)                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>615 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model = LlamaModel(config)                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">616 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">617 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>)        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">618 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\transformers\\mo</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">dels\\llama\\modeling_llama.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">446</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">443 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.vocab_size = config.vocab_size                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">444 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">445 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.pad   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>446 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.layers = nn.ModuleList([LlamaDecoderLayer(config) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> _ <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(config.num   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">447 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">448 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">449 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.gradient_checkpointing = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\transformers\\mo</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">dels\\llama\\modeling_llama.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">446</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;listcomp&gt;</span>                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">443 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.vocab_size = config.vocab_size                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">444 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">445 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.pad   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>446 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.layers = nn.ModuleList([LlamaDecoderLayer(config) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> _ <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(config.num   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">447 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">448 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">449 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.gradient_checkpointing = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\transformers\\mo</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">dels\\llama\\modeling_llama.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">256</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">253 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, config: LlamaConfig):                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">254 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>().<span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>()                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">255 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.hidden_size = config.hidden_size                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>256 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.self_attn = LlamaAttention(config=config)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">257 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.mlp = LlamaMLP(                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">258 │   │   │   </span>hidden_size=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.hidden_size,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">259 │   │   │   </span>intermediate_size=config.intermediate_size,                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\transformers\\mo</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">dels\\llama\\modeling_llama.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">178</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">175 │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\" and `num_heads`: {</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.num_heads<span style=\"color: #808000; text-decoration-color: #808000\">}).\"</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">177 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.q_proj = nn.Linear(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.hidden_size, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.num_heads * <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.head_dim, bias=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">F</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>178 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.k_proj = nn.Linear(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.hidden_size, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.num_heads * <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.head_dim, bias=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">F</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">179 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.v_proj = nn.Linear(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.hidden_size, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.num_heads * <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.head_dim, bias=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">F</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">180 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.o_proj = nn.Linear(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.num_heads * <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.head_dim, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.hidden_size, bias=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">F</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">181 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.rotary_emb = LlamaRotaryEmbedding(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.head_dim, max_position_embeddings=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">se</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\torch\\nn\\module</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">s\\linear.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">96</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 93 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>().<span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>()                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 94 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.in_features = in_features                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 95 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.out_features = out_features                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 96 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.weight = Parameter(torch.empty((out_features, in_features), **factory_kwarg   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 97 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> bias:                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 98 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias = Parameter(torch.empty(out_features, **factory_kwargs))             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 99 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span><span style=\"font-weight: bold\">[</span>enforce fail at \n",
       "C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">72</span><span style=\"font-weight: bold\">]</span> data. \n",
       "DefaultCPUAllocator: not enough memory: you tried to allocate <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">67108864</span> bytes.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\simone.marotta\\AppData\\Local\\Temp\\3\\ipykernel_12372\\3992398071.py\u001b[0m:\u001b[94m17\u001b[0m in \u001b[92m<module>\u001b[0m        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\simone.marotta\\\\AppData\\\\Local\\\\Temp\\\\3\\\\ipykernel_12372\\\\3992398071.py'\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\transformers\\mo\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mdeling_utils.py\u001b[0m:\u001b[94m2611\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2608 \u001b[0m\u001b[2m│   │   │   \u001b[0minit_contexts.append(init_empty_weights())                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2609 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2610 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m ContextManagers(init_contexts):                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2611 \u001b[2m│   │   │   \u001b[0mmodel = \u001b[96mcls\u001b[0m(config, *model_args, **model_kwargs)                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2612 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2613 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Check first if we are `from_pt`\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2614 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m use_keep_in_fp32_modules:                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\transformers\\mo\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mdels\\llama\\modeling_llama.py\u001b[0m:\u001b[94m615\u001b[0m in \u001b[92m__init__\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m612 \u001b[0m\u001b[94mclass\u001b[0m \u001b[4;92mLlamaForCausalLM\u001b[0m(LlamaPreTrainedModel):                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m613 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__init__\u001b[0m(\u001b[96mself\u001b[0m, config):                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m614 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96msuper\u001b[0m().\u001b[92m__init__\u001b[0m(config)                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m615 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.model = LlamaModel(config)                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m616 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m617 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=\u001b[94mFalse\u001b[0m)        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m618 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\transformers\\mo\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mdels\\llama\\modeling_llama.py\u001b[0m:\u001b[94m446\u001b[0m in \u001b[92m__init__\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m443 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.vocab_size = config.vocab_size                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m444 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m445 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, \u001b[96mself\u001b[0m.pad   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m446 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.layers = nn.ModuleList([LlamaDecoderLayer(config) \u001b[94mfor\u001b[0m _ \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(config.num   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m447 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m448 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m449 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.gradient_checkpointing = \u001b[94mFalse\u001b[0m                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\transformers\\mo\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mdels\\llama\\modeling_llama.py\u001b[0m:\u001b[94m446\u001b[0m in \u001b[92m<listcomp>\u001b[0m                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m443 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.vocab_size = config.vocab_size                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m444 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m445 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, \u001b[96mself\u001b[0m.pad   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m446 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.layers = nn.ModuleList([LlamaDecoderLayer(config) \u001b[94mfor\u001b[0m _ \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(config.num   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m447 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m448 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m449 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.gradient_checkpointing = \u001b[94mFalse\u001b[0m                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\transformers\\mo\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mdels\\llama\\modeling_llama.py\u001b[0m:\u001b[94m256\u001b[0m in \u001b[92m__init__\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m253 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__init__\u001b[0m(\u001b[96mself\u001b[0m, config: LlamaConfig):                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m254 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96msuper\u001b[0m().\u001b[92m__init__\u001b[0m()                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m255 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.hidden_size = config.hidden_size                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m256 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.self_attn = LlamaAttention(config=config)                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m257 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.mlp = LlamaMLP(                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m258 \u001b[0m\u001b[2m│   │   │   \u001b[0mhidden_size=\u001b[96mself\u001b[0m.hidden_size,                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m259 \u001b[0m\u001b[2m│   │   │   \u001b[0mintermediate_size=config.intermediate_size,                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\transformers\\mo\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mdels\\llama\\modeling_llama.py\u001b[0m:\u001b[94m178\u001b[0m in \u001b[92m__init__\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m175 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m and `num_heads`: \u001b[0m\u001b[33m{\u001b[0m\u001b[96mself\u001b[0m.num_heads\u001b[33m}\u001b[0m\u001b[33m).\u001b[0m\u001b[33m\"\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m176 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m177 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.q_proj = nn.Linear(\u001b[96mself\u001b[0m.hidden_size, \u001b[96mself\u001b[0m.num_heads * \u001b[96mself\u001b[0m.head_dim, bias=\u001b[94mF\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m178 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.k_proj = nn.Linear(\u001b[96mself\u001b[0m.hidden_size, \u001b[96mself\u001b[0m.num_heads * \u001b[96mself\u001b[0m.head_dim, bias=\u001b[94mF\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m179 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.v_proj = nn.Linear(\u001b[96mself\u001b[0m.hidden_size, \u001b[96mself\u001b[0m.num_heads * \u001b[96mself\u001b[0m.head_dim, bias=\u001b[94mF\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m180 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.o_proj = nn.Linear(\u001b[96mself\u001b[0m.num_heads * \u001b[96mself\u001b[0m.head_dim, \u001b[96mself\u001b[0m.hidden_size, bias=\u001b[94mF\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m181 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.rotary_emb = LlamaRotaryEmbedding(\u001b[96mself\u001b[0m.head_dim, max_position_embeddings=\u001b[96mse\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\torch\\nn\\module\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33ms\\linear.py\u001b[0m:\u001b[94m96\u001b[0m in \u001b[92m__init__\u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 93 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96msuper\u001b[0m().\u001b[92m__init__\u001b[0m()                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 94 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.in_features = in_features                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 95 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.out_features = out_features                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 96 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.weight = Parameter(torch.empty((out_features, in_features), **factory_kwarg   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 97 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m bias:                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 98 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.bias = Parameter(torch.empty(out_features, **factory_kwargs))             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 99 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0m\u001b[1m[\u001b[0menforce fail at \n",
       "C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:\u001b[1;36m72\u001b[0m\u001b[1m]\u001b[0m data. \n",
       "DefaultCPUAllocator: not enough memory: you tried to allocate \u001b[1;36m67108864\u001b[0m bytes.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline, LlamaForCausalLM, T5Tokenizer, AutoTokenizer, T5Model, AutoModelForSeq2SeqLM\n",
    "\n",
    "# model_path = \"D:\\\\hf_models\\\\flan-alpaca-large\"\n",
    "# tokenizer_path = \"D:\\\\hf_models\\\\flan-alpaca-large\"\n",
    "model_path = \"declare-lab/flan-alpaca-base\"\n",
    "tokenizer_path = \"declare-lab/flan-alpaca-base\"\n",
    "\n",
    "# tokenizer = T5Tokenizer(\n",
    "#     vocab_file=f\"{tokenizer_path}/tokenizer.json\",\n",
    "#     tokenizer_file=f\"{tokenizer_path}/tokenizer.json\",\n",
    "#     config_file=f\"{tokenizer_path}/tokenizer_config.json\",\n",
    "#     use_fast=False\n",
    "# )\n",
    "# model = LlamaForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "model = LlamaForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "llm = HuggingFacePipeline.from_model_id(model_id=\"bigscience/bloom-1b7\", task=\"text-generation\", model_kwargs={\"temperature\":0, \"max_length\":64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (64) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " First, we need to understand what is an electroencephalogram. An electroencephalogram is a recording of brain activity. It is a recording of brain activity that is made by placing electrodes on the scalp. The electrodes are placed\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate,  LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=hf)\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "local_path = 'D:\\gpt4all\\ggml-gpt4all-j-v1.3-groovy\\ggml-gpt4all-j-v1.3-groovy.bin'\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "# If you want to use GPT4ALL_J model add the backend parameter\n",
    "llm = GPT4All(model=local_path, backend='gptj', callbacks=callbacks, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### alpaca-native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 556/556 [00:00<00:00, 111kB/s]\n",
      "Downloading (…)model.bin.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 4.48MB/s]\n",
      "Downloading (…)l-00001-of-00003.bin: 100%|██████████| 9.88G/9.88G [02:12<00:00, 74.8MB/s]\n",
      "Downloading (…)l-00002-of-00003.bin: 100%|██████████| 9.89G/9.89G [03:38<00:00, 45.2MB/s]\n",
      "Downloading (…)l-00003-of-00003.bin: 100%|██████████| 7.18G/7.18G [01:57<00:00, 61.1MB/s]\n",
      "Downloading shards: 100%|██████████| 3/3 [08:20<00:00, 166.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\simone.marotta\\AppData\\Local\\Temp\\3\\ipykernel_10168\\2180908035.py\", line 5, in <module>\n",
      "    base_model = LlamaForCausalLM.from_pretrained(\n",
      "  File \"c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\transformers\\modeling_utils.py\", line 2626, in from_pretrained\n",
      "NameError: name 'init_empty_weights' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1055, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 955, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 778, in lines\n",
      "    return self._sd.lines\n",
      "  File \"c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"c:\\Users\\simone.marotta\\PycharmProjects\\chatbot-with-data\\venv\\lib\\site-packages\\executing\\executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"chavinlo/alpaca-native\")\n",
    "\n",
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "    \"chavinlo/alpaca-native\",\n",
    "    load_in_8bit=True,\n",
    "    device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m      3\u001b[0m pipe \u001b[39m=\u001b[39m pipeline(\n\u001b[0;32m      4\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtext-generation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m----> 5\u001b[0m     model\u001b[39m=\u001b[39mbase_model, \n\u001b[0;32m      6\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer, \n\u001b[0;32m      7\u001b[0m     max_length\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m,\n\u001b[0;32m      8\u001b[0m     temperature\u001b[39m=\u001b[39m\u001b[39m0.6\u001b[39m,\n\u001b[0;32m      9\u001b[0m     top_p\u001b[39m=\u001b[39m\u001b[39m0.95\u001b[39m,\n\u001b[0;32m     10\u001b[0m     repetition_penalty\u001b[39m=\u001b[39m\u001b[39m1.2\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m local_llm \u001b[39m=\u001b[39m HuggingFacePipeline(pipeline\u001b[39m=\u001b[39mpipe)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'base_model' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=base_model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=256,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\simone.marotta\\AppData\\Local\\Temp\\4\\ipykernel_2984\\1406395521.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">11</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\simone.marotta\\\\AppData\\\\Local\\\\Temp\\\\4\\\\ipykernel_2984\\\\1406395521.py'</span>              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">d:\\condivisi\\chatbot-with-data\\venv\\lib\\site-packages\\langchain\\chains\\retrieval_qa\\base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">94</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_chain_type</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 91 │   │   </span>combine_documents_chain = load_qa_chain(                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 92 │   │   │   </span>llm, chain_type=chain_type, **_chain_type_kwargs                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 93 │   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 94 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>(combine_documents_chain=combine_documents_chain, **kwargs)              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 95 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 96 │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@abstractmethod</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 97 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_get_docs</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, question: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>) -&gt; List[Document]:                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">d:\\condivisi\\chatbot-with-data\\pydantic\\main.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">341</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pydantic.main.BaseModel.__init__</span>          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: 'd:\\\\condivisi\\\\chatbot-with-data\\\\pydantic\\\\main.py'</span>       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValidationError: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> validation error for RetrievalQA\n",
       "prompt\n",
       "  extra fields not permitted <span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #800080; text-decoration-color: #800080\">value_error</span>.extra<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\simone.marotta\\AppData\\Local\\Temp\\4\\ipykernel_2984\\1406395521.py\u001b[0m:\u001b[94m11\u001b[0m in \u001b[92m<module>\u001b[0m         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\simone.marotta\\\\AppData\\\\Local\\\\Temp\\\\4\\\\ipykernel_2984\\\\1406395521.py'\u001b[0m              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33md:\\condivisi\\chatbot-with-data\\venv\\lib\\site-packages\\langchain\\chains\\retrieval_qa\\base.py\u001b[0m:\u001b[94m94\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mfrom_chain_type\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 91 \u001b[0m\u001b[2m│   │   \u001b[0mcombine_documents_chain = load_qa_chain(                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 92 \u001b[0m\u001b[2m│   │   │   \u001b[0mllm, chain_type=chain_type, **_chain_type_kwargs                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 93 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 94 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mcls\u001b[0m(combine_documents_chain=combine_documents_chain, **kwargs)              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 95 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 96 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;95m@abstractmethod\u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 97 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_get_docs\u001b[0m(\u001b[96mself\u001b[0m, question: \u001b[96mstr\u001b[0m) -> List[Document]:                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33md:\\condivisi\\chatbot-with-data\\pydantic\\main.py\u001b[0m:\u001b[94m341\u001b[0m in \u001b[92mpydantic.main.BaseModel.__init__\u001b[0m          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: 'd:\\\\condivisi\\\\chatbot-with-data\\\\pydantic\\\\main.py'\u001b[0m       \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValidationError: \u001b[0m\u001b[1;36m1\u001b[0m validation error for RetrievalQA\n",
       "prompt\n",
       "  extra fields not permitted \u001b[1m(\u001b[0m\u001b[33mtype\u001b[0m=\u001b[35mvalue_error\u001b[0m.extra\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "HUGGINGFACE_MODEL_NAME = \"google/flan-t5-base\"\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "llm = HuggingFaceHub(repo_id=HUGGINGFACE_MODEL_NAME, model_kwargs={\"temperature\":0, \"max_length\":512})\n",
    "# llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                    chain_type=\"stuff\",\n",
    "                                    retriever=vectorestore.as_retriever(),\n",
    "                                    input_key=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.run('How do you turn on the system?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "german_few_shot_doc_prompt = \"\"\"Given the following {summaries} of a long document and a {question}, create a final answer with references (\"SOURCES\").\"\"\"\n",
    "GERMAN_QA_PROMPT = PromptTemplate(template=german_few_shot_doc_prompt, input_variables=[\"summaries\", \"question\"])\n",
    "GERMAN_DOC_PROMPT = PromptTemplate(\n",
    "    template=\"Inhalt: {page_content}\\nQuelle: {source}\",\n",
    "    input_variables=[\"page_content\", \"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "qa_chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\",\n",
    "                                    prompt=GERMAN_QA_PROMPT,\n",
    "                                    document_prompt=GERMAN_DOC_PROMPT) \n",
    "chain = RetrievalQAWithSourcesChain(combine_documents_chain=qa_chain, retriever=vectorestore.as_retriever(),\n",
    "                                     reduce_k_below_max_tokens=True, max_tokens_limit=3375,\n",
    "                                     return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '\\n\\nAnswer: To turn on the system, press and hold the Power On button on the review module for 2 seconds. The system will take 5 minutes to become fully functional. If the system is not switched on, press and hold Power On on the review module until the indicator light stops flashing. To log off, select System from the menu bar of the review window, and then select Log Off. To switch the system off, press Power Off on the review module for 3 seconds.\\n\\nSources: Azurion Release 2.1 Instructions for Use, Philips 4522 203 78556, data\\\\e274394a0bfb4de196edae9a00adda36.pdf',\n",
       " 'sources': '',\n",
       " 'source_documents': [Document(page_content='4 Starting and Stopping the System\\nThis section provides information about starting and stopping the system during normal use. For\\ninformation about stopping the system in an emergency, see Emergency Stop  (page  20).\\nYou start and stop the system using the review module.\\nFigure 32 Review module\\nLegend\\n1 Power On\\n2 Power Off\\n3 Video Only\\n4.1 Starting the System\\n1On the review module, press and hold Power On  for 2 seconds.\\nNOTE Avoid operating any of the controls while the system is powering on, as this may inhibit\\nthe start-up process.\\n2Release the button when the indicator begins to flash.\\nThe indicator light stays on when the start-up process is complete.\\nFigure 33 System startup screenStarting and Stopping the System Starting the System\\nAzurion Release 2.1 Instructions for Use 51 Philips 4522 203 78556', metadata={'source': 'data\\\\e274394a0bfb4de196edae9a00adda36.pdf', 'page': 50}),\n",
       "  Document(page_content='The system takes 5 minutes from switching on until all functionality is available.\\n3If your work schedule includes tasks performed on a separate workstation, switch the workstation\\non and log on to it to avoid a delay during the procedure.\\n4When the logon screen appears, do the following:\\naClick the arrow in the User Name  box and select your user name.\\nbEnter your password in the Password  box.\\ncSelect Log On  or press Enter.\\nFigure 34 Logon screen\\nIf your password has expired, a dialog box is displayed allowing you to change your password. You\\nare asked to enter your existing password and to set your new password.\\n4.1.1 Accessing the System in an Emergency\\nDepending on the system configuration,  you can access the system in an emergency without logging\\non.\\n1If the system is not switched on, press and hold Power On  on the review module until the indicator\\nlight stops flashing.\\n2In the logon screen, click Emergency .\\nThe system is available in emergency access mode. This mode allow you to perform an emergency\\nprocedure, but has reduced functionality.\\nFor information about configuring  the system to allow emergency access, see Managing Users and\\nSystem Logon  (page  258) .\\n4.1.2 Switching On Only the Monitors (Option)\\nThis option allows you to use the monitors without switching the X-ray system on. You can then view\\nimages or perform a procedure that does not involve the system, such as ultrasound.\\nThis option is available if your system has the FlexVision or FlexSpot option installed, along with the\\nswitchable monitors option.\\n• Press Video Only  on the review module for at least 2 seconds.\\nThe monitors are switched on and the mouse is available for configuring  the screen layout.Starting and Stopping the System Starting the System\\nAzurion Release 2.1 Instructions for Use 52 Philips 4522 203 78556', metadata={'source': 'data\\\\e274394a0bfb4de196edae9a00adda36.pdf', 'page': 51}),\n",
       "  Document(page_content='4.2 Restarting the System\\nNOTE If control of the system starts to deviate from its expected behavior, you should restart the\\nsystem.\\nThere are two methods for restarting the system.\\n• Warm restart: Use this method when you are trying to resolve a software-related issue with the\\nsystem. This is the standard method for restarting the system.\\n• Cold restart: Use this method when you are trying to resolve a hardware-related issue with the\\nsystem.\\nWe recommend that you perform a cold restart of the system every day. During a cold restart important\\ndata is saved, which assists with remote servicing.\\nIf you stop the system using the emergency STOP  button, you must restart the system before you can\\nuse it again. For more information, see Emergency Stop  (page  20).\\n• To perform a warm restart, press and hold Power On  on the review module.\\nA warm restart takes 90 seconds until the system is fully functional. Fluoroscopy is possible after 60\\nseconds.\\nNOTE After a warm restart, X-ray is enabled.\\n• To perform a cold restart, do the following:\\naOn the review module, press and hold Power Off.\\nbRelease the button when the indicator light begins to flash.\\ncAfter the system has completely shut down, wait 10 seconds.\\ndOn the review module, press and hold Power On .\\nNOTE Do not operate any of the controls while the system is powering on, as this may\\ninhibit the start-up process.\\nA cold restart of the system takes 6 minutes from initiation of the cold restart until all system\\nfunctionality is available.\\n4.3 Mains Power Failure\\nThe system is powered by the hospital mains power supply. The stability of the mains power supply\\nmay vary over time and can sometimes be interrupted.\\nIn the event of a mains power failure, the system behaves as follows:\\n• All stored patient and system data is preserved.\\n• All mechanical, non-balanced movements are blocked.\\nIf a mains power failure occurs during a clinical procedure, you should do one of the following:\\n• Transport the patient to another system to continue the procedure.\\n• Wait until the hospital mains power supply is restored, and then restart the system to continue the\\nprocedure.\\nWhen the hospital’s back-up power system is active, the system takes measures to conserve power.\\nFunctions that cause high power consumption are disabled. Low-load fluoroscopy  is still possible, as\\nwell as patient and beam positioning functions. This ensures that you can always free the patient from\\nthe system.Starting and Stopping the System Restarting the System\\nAzurion Release 2.1 Instructions for Use 53 Philips 4522 203 78556', metadata={'source': 'data\\\\e274394a0bfb4de196edae9a00adda36.pdf', 'page': 52}),\n",
       "  Document(page_content='NOTE The last acquired series may be lost if the power failure occurs during the acquisition, or\\nshortly after the series was acquired.\\n4.3.1 Uninterruptible Power Supply (Option)\\nAn option is available to provide a limited amount of power to the system during mains power failure\\nusing an uninterruptible power supply (UPS).\\nThe optional uninterruptible power supply allows the system to perform a controlled shutdown if the\\nhospital mains power supply is interrupted. All data is backed up during the shutdown. For more\\ninformation, contact technical support.\\nOther compatible uninterruptible power supplies can be connected to the system, which allow full\\nfunctionality or reduced functionality for a limited time when mains power fails. However, such parts are\\nnot categorized as options for the system. For more information, contact technical support.\\n4.4 Restarting after Emergency Power Off\\nFollowing an emergency power off situation, the system will enter an emergency power off state.\\nThis is indicated by a flashing  indicator light above the Power On  button on the review module.\\nTo restart the system after an emergency power off situation, you must use the following procedure.\\n1When the indicator light above the Power On  button stops flashing,  press and hold Power On  for\\nmore than 2 seconds.\\n4.5 Stopping the System\\nSwitching the system off automatically logs you off. Alternatively, you can log off without switching the\\nsystem off, and leave the system available for the next operator.\\n• To log off, select System  from the menu bar of the review window, and then select Log Off.\\n• To switch the system off, press Power Off on the review module for 3 seconds.Starting and Stopping the System Restarting after Emergency Power Off\\nAzurion Release 2.1 Instructions for Use 54 Philips 4522 203 78556', metadata={'source': 'data\\\\e274394a0bfb4de196edae9a00adda36.pdf', 'page': 53})]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"question\": \"how to turn on the system?\"}, return_only_outputs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
